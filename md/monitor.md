# 智能监控平台首页大数据量优化复盘分析

## 难点1: 采集器回来的数据呈为垂直排列，一个监控数据代表一行数据，一个变压器至少监控20个数据点，5分钟检测从变压器中返回一次数据，在总部的图表需要加载所有的变压器数据，需要统计分析大量的变压器数据，查询效率很低

**思考/解决方案：**

1. 重新设计数据库的表结构逻辑：一个变压器5分钟仅返回一条数据，将垂直的数据重构为平行的数据（时间长后数据量依旧是很大，分库分表优化）
   + 首先将数据库由垂直的数据拆分为平行的数据
   + 新建一个点位表，点位表用于指定每个位置的表数据代表采集器的什么数据，可以在点位表里设置哪些是热点数据哪些是非热点数据，让热点数据先用redis进行预热
   + 使用netty与变压器进行协议的通信，使用了spring boot 2.x + netty4 的iot-modbus重构以往的结构

2. 为什么使用netty
   + 开发门槛低：api的使用比较简单
   + 定制能力强：可以通过ChannelHandler对通信框架进行灵活的扩展
   + Handler强大：预置了多种解码器，支持多种主流协议，对传输中粘包和拆包有现成的解决方案，有完善的预连，idle心跳机制等异常处理
   + 高性能：与现在主流的nio框架对比，netty的综合性能最优

3. 使用非关系型数据库对进行存储（主管提出的方案，并没有执行）

4. 需要进行读写分离，io比较频繁降低写入时占用查询的资源，让主数据库处理事务性增删改操作，让从数据库处理查询操作
   + 因为事务复制的技术复杂性比较低，对版本没有限制，只需要主从服务器能通过tcp通讯即可，所以使用事务复制的方法进行读写分离，对数据库的版本也不做限制
   + 主从搭建实际是 发布 - 分发 - 订阅的过程
5. 考虑分库分表的问题
   + io频繁：每5分钟新增大量数据（个别变压器需要1分钟记录大量数据），有大量的频繁新增数据的请求进入，读写分离解决这个问题
     + 拆分为主库和从库，主库为读写库，从库只读，我们使用复制模式进行配置，也称为发布-订阅模式，由主服务器进行发布信息，备份服务器进行订阅，当主服务器数据发生变更时，就会发布信息，备份服务器读取信息进行同步更新。（如果是表修改必须发布在属性中，需要手动同步结构）
     + 事务复制
       1. 使用主服务器配置分发
       2. 新建发布服务器，选择要发布的对象
       3. 新建订阅，在订阅服务器上运行每个代理，订阅数据库
       4. 新建向导配置
   + cpu瓶颈：单表数据量太大，查询时需要扫描的行太多，需要进行水平分表

6. 解决单表数据量过大的问题
   + 为什么不删除归档，将历史数据迁移到别的库中？公司需求出现卡顿时时间并不长，还没有将数据归档的需求。如果需要数据归档则先将需要删除的主键查询出来，并且需要代码通过动态id进行删除
   + 使用缓存，将redis放在db前面，先拦下一些高频读请求

### 分库分表策略

1. 水平分表，以字段为依据，使用的是每个变压器的编号进行，将一个库中的数据拆分到多个表中，

   + 系统的并发量不高，单表的数据量比较大，影响了sql的效率，加重了cpu的负担，以至于成为瓶颈。（当单表的数据少了，单次执行sql的效率高了，自然减轻了cpu的负担）
   + 还可以根据不同地区的变压器进行分库，根据地区下的客户公司进行分库处理

2. 进行垂直分表，系统的并发量不高，但是需要查询的点位并不是特别多，并且热点数据和非热点数据放在一起，单行数据存储的的空间较大，将热点的数据作为主表，非热点数据作为扩展表，比如电流电压负载率等情况放在主表中，其余读取回来的数据存放在扩展表中，减少随机读的io，要想获得全部的数据就必须关联两个表来进行查询，这个时候不能使用join，而在server层将两个表的数据查询出来之后再进行拼接处理

